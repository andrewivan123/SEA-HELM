#!/bin/bash
#PBS -P CFP01-CF-060
#PBS -j oe
#PBS -k oed
#PBS -N seahelm_eval
#PBS -q auto
#PBS -l select=1:ngpus=2
#PBS -l walltime=56:00:00

cd $PBS_O_WORKDIR

image="/app1/common/singularity-img/hopper/pytorch/pytorch_2.3.0_cuda_12.4_ngc_24.04.sif"

module load singularity

singularity exec -e \
--env HF_HOME=/scratch/e1583535/cache \
--env HF_DATASETS_CACHE=/scratch/e1583535/cache/datasets \
$image bash << EOF > /scratch/e1583535/multiLingual-llm-project/logs/eval/seahelm/stdout.$PBS_JOBID.log 2> /scratch/e1583535/multiLingual-llm-project/logs/eval/seahelm/stderr.$PBS_JOBID.log

if [ -d "/hpctmp/e1583535/virtualenvs/sea-helm" ]; then
    source /hpctmp/e1583535/virtualenvs/sea-helm/bin/activate
fi

echo "Starting evaluating at $(date)"
echo "Running on host: $(hostname)"
echo "GPU info:"
nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader,nounits || echo "No GPU detected"

# python /scratch/e1583535/SEA-HELM/seahelm_evaluation.py \
# --tasks translation_only \
# --output_dir /scratch/e1583535/multiLingual-llm-project/outputs/evaluation-model-200K-bin-shard/translation_only-rerun \
# --model_type vllm \
# --model_name /scratch/e1583535/llm/nus-olmo/para-last-n10B \
# --model_args "enable_prefix_caching=True,tensor_parallel_size=2,max_num_seqs=128" \
# --is_base_model \
# --num_in_context_examples 5

# python /scratch/e1583535/SEA-HELM/seahelm_evaluation.py \
# --tasks nlp_sentiment_nli_casual \
# --output_dir /scratch/e1583535/multiLingual-llm-project/outputs/evaluation-model-200K-bin-shard/nlp_sentiment_nli_casual \
# --model_type vllm \
# --model_name /scratch/e1583535/llm/nus-olmo/para-last-100B \
# --model_args "enable_prefix_caching=True,tensor_parallel_size=2,max_num_seqs=128" \
# --is_base_model \
# --num_in_context_examples 5

# python /scratch/e1583535/SEA-HELM/seahelm_evaluation.py \
# --tasks qa_5shot \
# --output_dir /scratch/e1583535/multiLingual-llm-project/outputs/evaluation-model-200K-bin-shard/qa_5shot \
# --model_type vllm \
# --model_name /scratch/e1583535/llm/nus-olmo/para-last-100B \
# --model_args "enable_prefix_caching=True,tensor_parallel_size=2,max_num_seqs=128" \
# --is_base_model \
# --num_in_context_examples 5

# python /scratch/e1583535/SEA-HELM/seahelm_evaluation.py \
# --tasks qa_1shot \
# --output_dir /scratch/e1583535/multiLingual-llm-project/outputs/evaluation-model-200K-bin-shard/qa_1shot-rerun \
# --model_type vllm \
# --model_name /scratch/e1583535/llm/nus-olmo/para-last-100B-step42931 \
# --model_args "enable_prefix_caching=True,tensor_parallel_size=2,max_num_seqs=128" \
# --is_base_model \
# --num_in_context_examples 1

# python /scratch/e1583535/SEA-HELM/seahelm_evaluation.py \
# --tasks abssum_5shot \
# --output_dir /scratch/e1583535/multiLingual-llm-project/outputs/evaluation-model-200K-bin-shard/abssum_5shot \
# --model_type vllm \
# --model_name /scratch/e1583535/llm/nus-olmo/para-last-100B \
# --model_args "enable_prefix_caching=True,tensor_parallel_size=2,max_num_seqs=128" \
# --is_base_model \
# --num_in_context_examples 5

# python /scratch/e1583535/SEA-HELM/seahelm_evaluation.py \
# --tasks abssum_1shot \
# --output_dir /scratch/e1583535/multiLingual-llm-project/outputs/evaluation-model-200K-bin-shard/abssum_1shot-rerun \
# --model_type vllm \
# --model_name /scratch/e1583535/llm/nus-olmo/para-last-100B \
# --model_args "enable_prefix_caching=True,tensor_parallel_size=2,max_num_seqs=128" \
# --is_base_model \
# --num_in_context_examples 1 \
# --rerun_cached_results

# python /scratch/e1583535/SEA-HELM/seahelm_evaluation.py \
# --tasks abssum_0shot \
# --output_dir /scratch/e1583535/multiLingual-llm-project/outputs/evaluation-model-200K-bin-shard/abssum_0shot \
# --model_type vllm \
# --model_name /scratch/e1583535/llm/nus-olmo/para-last-n10B-rerun \
# --model_args "enable_prefix_caching=True,tensor_parallel_size=2,max_num_seqs=128" \
# --is_base_model \
# --num_in_context_examples 0 \
# --rerun_cached_results

# python /scratch/e1583535/SEA-HELM/seahelm_evaluation.py \
# --tasks abssum_2shot \
# --output_dir /scratch/e1583535/multiLingual-llm-project/outputs/evaluation-model-200K-bin-shard/abssum_2shot \
# --model_type vllm \
# --model_name /scratch/e1583535/llm/nus-olmo/para-last-100B \
# --model_args "enable_prefix_caching=True,tensor_parallel_size=2,max_num_seqs=128" \
# --is_base_model \
# --num_in_context_examples 2

EOF